[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "",
    "text": "Welcome to the Design Document for CPU parallelism in NumPy, SciPy, scikit-learn, and pandas. Each library has varying levels of support for running parallel computation. This document details the current status of parallelism with shipping code on PyPI and possible paths for improvement."
  },
  {
    "objectID": "index.html#apis-for-configuring-parallelism",
    "href": "index.html#apis-for-configuring-parallelism",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "APIs for Configuring Parallelism",
    "text": "APIs for Configuring Parallelism\nThere are three ways to configure parallelism across the libraries: environment variables, threadpoolctl, or library-specific Python API.\nExamples of environment variables consist of:\n\nOPENBLAS_NUM_THREADS for OpenBLAS\nMKL_NUM_THREADS for MKL\nOMP_NUM_THREADS for OpenMP\n\nThese environment variables control how many threads a specific backend uses. These environment variables do not influence code that does not use a particular backend, like OpenMP. For example, SciPy’s fft module uses pthreads directly.\nThreadpoolctl provides a Python interface for configuring the number of threads in OpenBLAS, MKL, and OpenMP. Linear algebra function calls from NumPy, SciPy, or scikit-learn can all be configured with threadpoolctl or an environment variable.\nSciPy and scikit-learn have a library-specific Python API for controlling parallelism. SciPy’s workers can mean multithreading, multiprocessing, or pthreads. Scikit-learn’s n_jobs is either multiprocessing or multithreading. threadpoolctl or OMP_NUM_THREADS configured scikit-learn’s computation routines that use OpenMP. Note that scikit-learn’s n_jobs does not configure OpenMP or OpenBLAS parallelism.\n\nProposal\nHere is a two step proposal:\n\nDocument the functions or methods using OpenMP or BLAS and can be configured with an environment variable or threadpoolctl.\nAdopt a consistent Python API for configuring parallelism. We use SciPy’s workers parameter because it is more consistent in controlling the number of cores used. workers denotes any form of parallalism including: multi-threading, multiprocessing, OpenMP threads, or pthreads.\nIf a library has nested parallal calls, it will configure the computational routines to prevent subscription."
  },
  {
    "objectID": "index.html#multi-threaded-by-default",
    "href": "index.html#multi-threaded-by-default",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "Multi-threaded by Default",
    "text": "Multi-threaded by Default\nBLAS implementations such as OpenBLAS are multi-threaded by default. Scikit-learn followed this convention with OpenMP, which is also multi-threaded by default. Using all the CPU cores by default is convenient for interactive use cases like in a Jupyter Notebook. The downside of using all CPU cores is during deployment to shared environments. The user needs to know which API to configure their program to become serial from the above section.\nThere is oversubscription when using Python multiprocessing or multi-thraeding together with OpenBLAS or OpenMP which uses multiple threads. Distributed Python libraries such as Dask and Ray recommend setting environment variables to configure OpenBLAS and OpenMP to run serially.\n\nProposal\nHere are some possible paths we can take:\n\nKeep the status quo where BLAS is multi-threaded by default. SciPy’s linalg module or scikit-learn’s OpenMP accelerated routines will continue to be parallel as the default.\nLibraries all have a serial fallback and we only ship the serial form on PyPi. We encourge OpenMP anywhere the whole stack is built in in a consistent fashion.\nMigrate from multi-threaded to single-thread as the default. Each library has the option to include a global flag that configures all computations to be parallel.\n\nOptions 2 and 3 helps with oversubsctiption because the library is serial by default."
  },
  {
    "objectID": "index.html#interactions-between-different-forms-of-parallelism",
    "href": "index.html#interactions-between-different-forms-of-parallelism",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "Interactions Between Different Forms of Parallelism",
    "text": "Interactions Between Different Forms of Parallelism\nWhen different parallelism interfaces are running concurrently, it is possible to run into crashes or oversubscription. The following is a list of known issues:\n\nlibgomp (OpenMP for GCC) is not fork-safe while libomp (OpenMP for Clang) is fork-safe. Scikit-learn has developed loky as a workaround. There is a patch to GCC OpenMP to make it fork safe, but it has not progressed. For details, see scikit-learn’s FAQ entry.\nlibomp (OpenMP for Clang) not compatible with libiomp (OpenMP for Intel Complier). The workaround is to set MKL_THREADING_LAYER=GNU. See this link for details.\nlibgomp (OpenMP for GCC) is also not compatible with libiomp (OpenMP for Intel Complier): pytorch#37377\nThere are performance issues when OpenBLAS (built with pthreads) and OpenMP have separate threadpools: OpenBLAS#3187. A workaround is to share the same threadpool by building OpenBLAS with OpenMP.\nThere are performance issues when two OpenBLAS are present, such as in NumPy and SciPy: scipy#15129. The current workaround is to set OPENBLAS_THREAD_TIMEOUT=1 on the affected platforms.\n\n\nProposal\nThe following are feasible steps we can take to improve the issues listed above:\n\nThe library sends a warning or error to notify the user when a known issue is detected. For example, Numba detects when libgomp and fork are used together, raising an error.\nThe library detects and raises a warning recommending MKL_THREADING_LAYER when LLVM OpenMP and Intel OpenMP are loaded together. For example, threadpoolctl has such a warning.\nMove towards a single OpenMP and OpenBLAS on PyPI by shipping an OpenMP and OpenBLAS wheel. NumPy, SciPy, and Scikit-learn will link to those libraries during runtime, thus allowing the libraries to share the same threadpool. Please see the technical details section on how this could work."
  },
  {
    "objectID": "index.html#how-to-ship-openmp-and-openblas-on-pypi",
    "href": "index.html#how-to-ship-openmp-and-openblas-on-pypi",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "How to ship OpenMP and OpenBLAS on PyPI?",
    "text": "How to ship OpenMP and OpenBLAS on PyPI?\nOpenMP and OpenBLAS are shipped wheels with their header files. When building an upstream library such as NumPy, extensions will use RPATH to link to the OpenMP and OpenBLAS wheels. auditwheel repair needs a patch so that it does not copy PyPi libraries into the wheel: auditwheel#392. Note that PEP513, explicitly allows for shared libraries to be distributed as separate packages on PyPI."
  },
  {
    "objectID": "index.html#which-compiler-to-use-for-openmp",
    "href": "index.html#which-compiler-to-use-for-openmp",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "Which compiler to use for OpenMP?",
    "text": "Which compiler to use for OpenMP?\nThere are two options: libgomp (OpenMP for GCC) or libomp (Clang for GCC).\n\nlibgomp is not fork safe, but uses the GCC and shipped with all Linux distros. We advocate for the patch in GCC to make it fork safe.\nlibomp is fork safe, but it is an implementation detail and not part of the OpenMP specification.\n\nOn PyPI, I propose we go with libomp, because it has the same symbols as libgomp and is fork safe. Upstream libraries such as NumPy or SciPy can still use GCC as their compiler. Package managers can still ship libraries linked with libgomp. SciPy has an existing discussion regarding OpenMP adoption and the compiler choice: scipy#10239."
  },
  {
    "objectID": "index.html#with-a-openmp-wheel-can-workers-configure-and-use-another-threading-api-like-pthreads",
    "href": "index.html#with-a-openmp-wheel-can-workers-configure-and-use-another-threading-api-like-pthreads",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "With a OpenMP wheel, can workers configure and use another threading API like pthreads?",
    "text": "With a OpenMP wheel, can workers configure and use another threading API like pthreads?\nYes, this design documentation does not restrict the usage of other API for parallalism. The only requirement is that if your library uses OpenMP, then link it to the OpenMP wheel. This way all libraries can share the same OpenMP thread pool."
  },
  {
    "objectID": "index.html#how-does-conda-forge-work",
    "href": "index.html#how-does-conda-forge-work",
    "title": "Parallalism in Numerical Python Libraries",
    "section": "How does conda-forge work?",
    "text": "How does conda-forge work?\nFor BLAS, conda-forge builds with netlib. During installation time, BLAS can be switched to other implementations such as MKL, BLIS, OpenBLAS. See this link for details.\nFor OpenMP, conda-forge builds with libgomp, the GNU build of OpenMP. During installation time, OpenMP can be switched to libomp, the LLVM build of OpenMP. Recall that the LLVM implementation is fork-safe. Note, that the GNU implementation has target offloading symbols, while LLVM does not. See this link for details.\nConda-forge has a mutex package ensuring that a single OpenMP or BLAS library is installed and loaded."
  }
]